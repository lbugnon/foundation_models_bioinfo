{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VcSkCEsmWMZ9"
   },
   "source": [
    "The idea is to explore residues representation in LLM models. Can we get conclussions from aminoacid properties?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uOPsvo8mVegI",
    "outputId": "228a0090-4b82-4c9b-f4b2-ace6ac053e6c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/facebookresearch/esm/zipball/main\" to /root/.cache/torch/hub/main.zip\n",
      "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t33_650M_UR50D.pt\" to /root/.cache/torch/hub/checkpoints/esm2_t33_650M_UR50D.pt\n",
      "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/regression/esm2_t33_650M_UR50D-contact-regression.pt\" to /root/.cache/torch/hub/checkpoints/esm2_t33_650M_UR50D-contact-regression.pt\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch as tr\n",
    "import pandas as pd\n",
    "\n",
    "data_path = \"https://raw.githubusercontent.com/lbugnon/foundation_models_bioinfo/refs/heads/main/data/\"\n",
    "df = pd.read_csv(f\"{data_path}/pfam_some_sequences.csv\", index_col=\"sequence_name\")\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "MAX_LEN = 1022\n",
    "LAST_LAYER = 30\n",
    "\n",
    "# https://github.com/facebookresearch/esm#available-models\n",
    "model, alphabet = tr.hub.load(\"facebookresearch/esm:main\",\n",
    "                              \"esm2_t33_650M_UR50D\")\n",
    "                              #\"esm2_t30_150M_UR50D\")\n",
    "model.eval()\n",
    "model.to(DEVICE)\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "\n",
    "def get_embedding(seq, aggregate=False):\n",
    "    \"\"\"Recibe una secuencia, devuelve un tensor de MxL donde M es el tamaño\n",
    "    del embedding y L la longitud de la secuencia. Si aggregate=True, promedia\n",
    "    la representación a lo largo de la secuencia devolviendo un vector M\"\"\"\n",
    "\n",
    "    # Recorta el dominio si supera la capacidad del LLM\n",
    "    center = len(seq)//2\n",
    "    start = max(0, center - MAX_LEN//2)\n",
    "    end = min(len(seq), center + MAX_LEN//2)\n",
    "    seq = seq[start:end]\n",
    "\n",
    "    # Formato requerido por el tokenizador, se pueden procesar  por lotes\n",
    "    # en paralelo\n",
    "    x = [(0, seq)]\n",
    "\n",
    "    with tr.no_grad():\n",
    "        _, _, tokens = batch_converter(x)\n",
    "        emb = model(tokens.to(DEVICE), repr_layers=[LAST_LAYER])[\"representations\"][LAST_LAYER].detach().cpu().squeeze()\n",
    "\n",
    "    emb = emb.permute(1,0) # [L, M] -> [M, L], por conveniencia mas adelante\n",
    "\n",
    "    if aggregate:\n",
    "        emb = np.array(emb.mean(dim=1))\n",
    "\n",
    "    return emb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1kV47FnvWtKQ"
   },
   "source": [
    "sample a number of sequences from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rCskzC1AWo5P"
   },
   "outputs": [],
   "source": [
    "sequences = df.sample(100).sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1tJ6sFCsWv1J",
    "outputId": "6b0696e9-af08-4607-bf15-057b516a354f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of AA: 20\n",
      "AA count:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'A: 1986',\n",
       " 'C: 285',\n",
       " 'D: 1227',\n",
       " 'E: 1304',\n",
       " 'F: 1029',\n",
       " 'G: 1534',\n",
       " 'H: 444',\n",
       " 'I: 1356',\n",
       " 'K: 1136',\n",
       " 'L: 2376',\n",
       " 'M: 515',\n",
       " 'N: 998',\n",
       " 'P: 1034',\n",
       " 'Q: 811',\n",
       " 'R: 1231',\n",
       " 'S: 1523',\n",
       " 'T: 1261',\n",
       " 'V: 1523',\n",
       " 'W: 317',\n",
       " 'Y: 870'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get counts for each aminoacid\n",
    "aminoacids = sequences.str.cat()\n",
    "print(f\"number of AA: {len(set(aminoacids))}\")\n",
    "print(\"AA count:\")\n",
    "aa_count = {f\"{aa}: {aminoacids.count(aa)}\" for aa in set(aminoacids)}\n",
    "aa_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PsMAdcDyXcfc"
   },
   "source": [
    "Now we want to get a numeric representation of each residue of the sequences and get the average representation for each AA.\n",
    "- why average?\n",
    "- why we can't just compute the embedding for the 20 AAs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aqoOUwlZWxB7"
   },
   "outputs": [],
   "source": [
    "aa_embeddings = {}\n",
    "\n",
    "for seq in sequences:\n",
    "    emb_seq = get_embedding(seq, aggregate=False)\n",
    "\n",
    "    # TODO get representation for each aminoacid and sum them\n",
    "    \n",
    "\n",
    "# normalize embedding sum\n",
    "for aa in aa_embeddings:\n",
    "    aa_embeddings[aa] /= aminoacids.count(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4S8MHShVXfEU"
   },
   "outputs": [],
   "source": [
    "# sorting all\n",
    "aa_names = list(aa_embeddings.keys())\n",
    "aa_embeddings = np.array([aa_embeddings[aa] for aa in aa_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tk78htWXXlI9"
   },
   "outputs": [],
   "source": [
    "# use pca instead of umap\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "emb2D = pca.fit_transform(aa_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 540
    },
    "id": "qtkGj5iKXmkc",
    "outputId": "75a32fc0-31c6-4aa2-833a-f4e2475027ec"
   },
   "outputs": [],
   "source": [
    "#!pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# get some aminoacids properties\n",
    "negatively_charged = ['D', 'E']\n",
    "positively_charged = ['R', 'K', 'H']\n",
    "polar = ['Q', 'N', 'S', 'T']\n",
    "unique = ['C', 'G', 'P']\n",
    "aromatic = ['F', 'Y', 'W']\n",
    "hydrophobic = ['A', 'V', 'L', 'I', 'M']\n",
    "small = ['A', 'G', 'S', 'T', 'V']\n",
    "large = ['K', 'R', 'D', 'E', 'Q', 'N']\n",
    "\n",
    "\n",
    "# plot properties\n",
    "plt.figure(figsize=(12, 6))\n",
    "for k, aa in enumerate(aa_names):\n",
    "    s = 50\n",
    "    if aa in small:\n",
    "        s = 30\n",
    "    elif aa in large:\n",
    "        s = 100\n",
    "\n",
    "    if aa in negatively_charged:\n",
    "        plt.scatter(emb2D[k, 0], emb2D[k, 1], label=aa, s=s, marker=\"x\", color='red');\n",
    "    elif aa in positively_charged:\n",
    "        plt.scatter(emb2D[k, 0], emb2D[k, 1], label=aa, s=s, marker=\"s\", color='red');\n",
    "    elif aa in polar:\n",
    "        plt.scatter(emb2D[k, 0], emb2D[k, 1], label=aa, s=s, color='blue');\n",
    "    elif aa in unique:\n",
    "        plt.scatter(emb2D[k, 0], emb2D[k, 1], label=aa, s=s, color='orange');\n",
    "    elif aa in hydrophobic:\n",
    "        plt.scatter(emb2D[k, 0], emb2D[k, 1], label=aa, s=s, marker=\"o\", color='green');\n",
    "    elif aa in aromatic:\n",
    "        plt.scatter(emb2D[k, 0], emb2D[k, 1], label=aa, s=s, marker=\"+\", color='green');\n",
    "\n",
    "\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
